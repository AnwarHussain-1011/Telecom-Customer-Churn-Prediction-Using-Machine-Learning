# -*- coding: utf-8 -*-
"""Untitled41.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1poO03x6PuebMiuaBuEl54SdwrKcBmjNU
"""

# ================================
# Step 1: Install & Import Libraries
# ================================
!pip install seaborn plotly scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from google.colab import files

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# ================================
# Step 2: Upload & Load Dataset
# ================================
uploaded = files.upload()  # Upload the CSV file manually in Colab
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

# ================================
# Step 3: Data Preprocessing
# ================================
df.drop(columns=['customerID'], inplace=True)  # Drop customerID since it's not useful for ML

# Convert 'TotalCharges' to numeric, handling missing values
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan)  # Convert blank spaces to NaN
df['TotalCharges'] = df['TotalCharges'].astype(float)

# Fill missing TotalCharges with median value
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Encoding categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

# ================================
# Step 4: Data Visualization (EDA)
# ================================
plt.figure(figsize=(6,4))
sns.countplot(x=df["Churn"])
plt.title("Customer Churn Distribution")
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df["Churn"], y=df["MonthlyCharges"])
plt.title("Monthly Charges vs Churn")
plt.show()

# ================================
# Step 5: Train-Test Split
# ================================
X = df.drop(columns=['Churn'])  # Features
y = df['Churn']  # Target variable

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ================================
# Step 6: Train Machine Learning Models
# ================================

models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "AdaBoost": AdaBoostClassifier(n_estimators=50, random_state=42)
}

# Train and Evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"\nüîπ Model: {name}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

"""*2nd method step by step*"""

# ================================
# Step 1: Install & Import Libraries
# ================================
!pip install seaborn plotly scikit-learn xgboost tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from google.colab import files

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

import tensorflow as tf
from tensorflow import keras

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# ================================
# Step 2: Upload & Load Dataset
# ================================
uploaded = files.upload()  # Upload the CSV file manually in Colab
df = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")

# ================================
# Step 3: Data Preprocessing
# ================================
df.drop(columns=['customerID'], inplace=True)  # Drop customerID since it's not useful for ML

# Convert 'TotalCharges' to numeric, handling missing values
df['TotalCharges'] = df['TotalCharges'].replace(" ", np.nan)  # Convert blank spaces to NaN
df['TotalCharges'] = df['TotalCharges'].astype(float)
df['TotalCharges'].fillna(df['TotalCharges'].mean(), inplace=True)  # Fill NaN values with mean

# Encoding categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    label_encoders[col] = LabelEncoder()
    df[col] = label_encoders[col].fit_transform(df[col])

# ================================
# Step 4: Exploratory Data Analysis (EDA)
# ================================
plt.figure(figsize=(6,4))
sns.countplot(x=df["Churn"])
plt.title("Customer Churn Distribution")
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df["Churn"], y=df["MonthlyCharges"])
plt.title("Monthly Charges vs Churn")
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x="gender", hue="Churn", data=df)
plt.title("Gender and Churn Distribution")
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x="PaymentMethod", hue="Churn", data=df)
plt.title("Payment Method and Churn Distribution")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), cmap='coolwarm', annot=False)
plt.title("Feature Correlation Heatmap")
plt.show()

# ================================
# Step 5: Train-Test Split & Data Scaling
# ================================
X = df.drop(columns=['Churn'])  # Features
y = df['Churn']  # Target variable

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# ================================
# Step 6: Train Machine Learning Models
# ================================

models = {
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "AdaBoost": AdaBoostClassifier(n_estimators=50, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

# Train and Evaluate each model
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    results.append((name, acc))

    print(f"\nüîπ Model: {name}")
    print(f"Accuracy: {acc:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# ================================
# Step 7: Hyperparameter Tuning (GridSearchCV for RandomForest)
# ================================
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("\nüîπ Best Parameters for Random Forest:", grid_search.best_params_)

# ================================
# Step 8: Deep Learning Model (Neural Network)
# ================================
model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print("\nüîπ Deep Learning Model Accuracy:", accuracy)

# ================================
# Step 9: Model Performance Comparison
# ================================
results.append(("Neural Network", accuracy))
df_results = pd.DataFrame(results, columns=["Model", "Accuracy"])
df_results = df_results.sort_values(by="Accuracy", ascending=False)

plt.figure(figsize=(10,5))
sns.barplot(x=df_results["Accuracy"], y=df_results["Model"], palette="coolwarm")
plt.title("Model Accuracy Comparison")
plt.xlabel("Accuracy")
plt.ylabel("Machine Learning Model")
plt.show()

# ================================
# Step 10: Save the Best Model
# ================================
best_model_name, best_model_acc = df_results.iloc[0]
print(f"\nüèÜ Best Performing Model: {best_model_name} with Accuracy: {best_model_acc:.4f}")